"""
References and Citation Analysis Module.

This module evaluates the quality of the references section of a scientific paper based on a set of heuristics.
It focuses on:
- Quantity of references.
- Recency of sources.
- Consistency of formatting.
- Diversity of source types.
"""

import re
import logging
from datetime import datetime
from typing import List, Dict, Tuple
from collections import Counter

from models.paper import Paper, Reference
from models.score import PillarResult

# Setup logger
logger = logging.getLogger(__name__)

# -------------------- METRIC IMPLEMENTATIONS --------------------

def _check_reference_quantity(references: List[Reference]) -> Tuple[float, str]:
    """Scores the quantity of references."""
    count = len(references)
    
    if count < 10:
        score = 0.3
        feedback = f"‚ö†Ô∏è The paper has only {count} references, which is too few for a scientific publication."
    elif 10 <= count < 15:
        score = 0.7
        feedback = f"‚úÖ The number of references ({count}) is acceptable, though slightly low."
    elif 15 <= count <= 60:
        score = 1.0
        feedback = f"‚úÖ The paper has a healthy number of references ({count})."
    else:  # count > 60
        score = 0.8
        feedback = f"‚ö†Ô∏è The paper has a high number of references ({count}), which could suggest a lack of focus."
    
    return score, feedback


def _check_reference_recency(references: List['Reference']) -> Tuple[float, str]:
    """Scores the recency of references based on detected publication years."""
    years = []
    current_year = datetime.now().year

    for ref in references:
        # Buscar todos los a√±os v√°lidos entre 1950 y el a√±o actual
        found = re.findall(r'\b(19[5-9]\d|20[0-4]\d|2050)\b', ref.text)
        # Escoger el a√±o m√°s reciente encontrado en esa referencia
        if found:
            years.append(max(map(int, found)))

    if not years:
        return 0.3, "‚ö†Ô∏è Could not determine the publication year for most references."

    # Calcular la proporci√≥n de referencias con menos de 10 a√±os
    recent_count = sum(1 for y in years if y >= current_year - 10)
    ratio = recent_count / len(years)

    # Asignar puntuaci√≥n y feedback
    if ratio >= 0.7:
        score = 1.0
        feedback = f"‚úÖ A high proportion ({int(ratio * 100)}%) of references are from the last 10 years."
    elif ratio >= 0.4:
        score = 0.7
        feedback = f"üü° A reasonable proportion ({int(ratio * 100)}%) of references are recent."
    else:
        score = 0.4
        feedback = f"‚ö†Ô∏è Many references seem outdated. Only {int(ratio * 100)}% are from the last 10 years."

    return score, feedback


def _check_format_consistency(references: List[Reference]) -> Tuple[float, str]:
    """Scores the consistency of the reference format using heuristics."""
    patterns = []
    for ref in references:
        text = ref.text.strip()
        # Pattern 1: Starts with [1] or (1)
        if re.match(r'^[\(\[]\d+[\)\]]', text):
            patterns.append("numbered")
        # Pattern 2: Starts with an author name (e.g., "Smith, J.")
        elif re.match(r'^[A-Z][a-z]+,\s*[A-Z]\.', text):
            patterns.append("author_year")
        else:
            patterns.append("unknown")

    if not patterns:
        return 0.2, "‚ö†Ô∏è Could not identify a consistent citation format."

    # Find the most common pattern
    most_common = Counter(patterns).most_common(1)[0]
    dominant_pattern, count = most_common
    consistency_ratio = count / len(references)

    if dominant_pattern == "unknown":
        return 0.3, "‚ö†Ô∏è The reference format is highly inconsistent or uses a non-standard style."
    
    if consistency_ratio >= 0.85:
        score = 1.0
        feedback = "‚úÖ The reference format appears to be consistent."
    elif consistency_ratio >= 0.6:
        score = 0.6
        feedback = "‚ö†Ô∏è The reference format shows some inconsistencies. Please verify all entries follow the same style."
    else:
        score = 0.3
        feedback = "‚ùå The reference format is highly inconsistent. A standard citation style should be applied uniformly."
    return score, feedback


def _check_source_diversity(references: List[Reference]) -> Tuple[float, str]:
    """Scores the diversity of reference sources."""
    source_types = []
    for ref in references:
        text = ref.text.lower()
        if any(keyword in text for keyword in ["journal", "transactions", "proceedings", "conference"]):
            source_types.append("journal/conference")
        elif "book" in text or "press" in text:
            source_types.append("book")
        elif "http" in text or "www" in text:
            source_types.append("web")
        else:
            source_types.append("other")
    
    counts = Counter(source_types)
    num_types = len(counts)

    if num_types >= 3:
        score = 1.0
        feedback = "‚úÖ Good diversity of sources (journals, books, web, etc.)."
    elif num_types == 2:
        score = 0.8
        feedback = "‚úÖ Source diversity is adequate, mainly consisting of two types."
    else:
        score = 0.5
        feedback = "‚ö†Ô∏è Source diversity could be improved. Most references are of the same type."

    # Penalize if web sources are dominant
    if len(references) > 0 and counts.get("web", 0) / len(references) > 0.5:
        score *= 0.7
        feedback += " However, there is a high reliance on web sources, which may not always be peer-reviewed."
        
    return score, feedback


# -------------------- MAIN EVALUATION LOGIC --------------------

def evaluate(paper: Paper, use_gpt: bool = False) -> dict:
    """
    Evaluates the quality of the paper's references.

    This function provides a baseline analysis of the references, which can be
    later enhanced by an LLM analysis (Perplexity Sonar).
    """
    references = paper.references

    if not references:
        feedback = "The paper does not contain a references section or it could not be parsed."
        result = PillarResult("References & Citations", 0.1, feedback).__dict__
        
        if use_gpt:
            result['gpt_analysis'] = {
                "used": False,
                "success": False,
                "reason": "No references found to analyze",
                "error": "Skipped due to no references."
            }
        return result

    # 1. Evaluate each metric
    quantity_score, quantity_feedback = _check_reference_quantity(references)
    recency_score, recency_feedback = _check_reference_recency(references)
    consistency_score, consistency_feedback = _check_format_consistency(references)
    diversity_score, diversity_feedback = _check_source_diversity(references)

    # 2. Calculate weighted overall score
    weights = {
        'quantity': 0.25,
        'recency': 0.30,
        'consistency': 0.30,
        'diversity': 0.15
    }
    
    overall_score = (
        quantity_score * weights['quantity'] +
        recency_score * weights['recency'] +
        consistency_score * weights['consistency'] +
        diversity_score * weights['diversity']
    )
    overall_score = round(overall_score, 2)

    # 3. Generate combined feedback
    feedback = "\n  ".join([
        quantity_feedback,
        recency_feedback,
        consistency_feedback,
        diversity_feedback
    ])

    result = PillarResult("References & Citations", overall_score, feedback).__dict__
    
    # Add score breakdown
    result['score_breakdown'] = {
        'quantity': quantity_score,
        'recency': recency_score,
        'consistency': consistency_score,
        'diversity': diversity_score
    }

    # ‚úÖ CORRECCI√ìN: An√°lisis con Perplexity si use_gpt est√° activado
    if use_gpt:
        logger.info("Attempting Perplexity analysis for references...")
        try:
            # Intentar diferentes formas de importar
            try:
                from integrations.perplexity_api import analyze_references
                logger.info("Successfully imported perplexity_api")
            except ImportError as e:
                logger.warning(f"First import attempt failed: {e}")
                import sys
                from pathlib import Path
                # A√±adir el directorio src al path si no est√°
                src_path = Path(__file__).parent.parent
                if str(src_path) not in sys.path:
                    sys.path.insert(0, str(src_path))
                    logger.info(f"Added {src_path} to sys.path")
                from integrations.perplexity_api import analyze_references
                logger.info("Successfully imported perplexity_api on second attempt")
            
            # Llamar a Perplexity
            logger.info(f"Calling Perplexity API with {len(references)} references...")
            perplexity_result = analyze_references(references)
            logger.info(f"Perplexity returned: success={perplexity_result.get('success')}")
            if not perplexity_result.get('success'):
                logger.error(f"Perplexity analysis error: {perplexity_result.get('error')}")

            # ‚úÖ Asegurar estructura correcta con todos los campos necesarios
            result['gpt_analysis'] = {
                'used': True,
                'success': perplexity_result.get('success', False),
                'analysis': perplexity_result.get('analysis', ''),
                'model': perplexity_result.get('model', 'perplexity-sonar'),
                'cost_info': perplexity_result.get('cost_info', {})
            }
            
            # A√±adir error si fall√≥
            if not perplexity_result.get('success'):
                result['gpt_analysis']['error'] = perplexity_result.get('error', 'Unknown error')
            
            # Si tuvo √©xito, mejorar el feedback b√°sico
            if perplexity_result.get('success'):
                analysis_text = perplexity_result.get('analysis', '')
                if analysis_text:
                    result['feedback'] += f"\n\n--- PERPLEXITY SONAR ANALYSIS ---\n{analysis_text}"
                    
        except ImportError as e:
            logger.error(f"Failed to import perplexity_api: {e}")
            result['gpt_analysis'] = {
                "used": False,
                "success": False,
                "reason": "Perplexity integration not installed",
                "error": f"The perplexity_api module could not be imported: {str(e)}"
            }
        except Exception as e:
            logger.error(f"Perplexity analysis failed: {e}")
            result['gpt_analysis'] = {
                "used": True,
                "success": False,
                "error": f"Perplexity analysis failed: {str(e)}"
            }
            
    return result