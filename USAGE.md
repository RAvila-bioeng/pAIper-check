# pAIper Check - GuÃ­a de Uso

## InstalaciÃ³n

1. Instalar dependencias:
```bash
pip install -r requirements.txt
```

2. Configurar variables de entorno (opcional):
```bash
# Crear archivo .env
OPENAI_API_KEY=tu_api_key_aqui
MONGO_URI=tu_mongo_uri_aqui
DEBUG=false
LOG_LEVEL=INFO
```

## Uso BÃ¡sico

### Evaluar un paper
```bash
python src/main.py --input data/sample_paper_1.txt
```

### Evaluar con salida verbose
```bash
python src/main.py --input data/sample_paper_1.txt --verbose
```

### Guardar resultados en archivo JSON
```bash
python src/main.py --input data/sample_paper_1.txt --output results.json
```

## MÃ³dulos de EvaluaciÃ³n

El sistema evalÃºa papers en 6 pilares principales:

### 1. Structure & Completeness (15%)
- Verifica presencia de secciones esenciales
- EvalÃºa estructura del documento
- Analiza calidad del tÃ­tulo

### 2. Linguistic Quality (15%)
- DetecciÃ³n de errores ortogrÃ¡ficos
- Consistencia terminolÃ³gica
- Estilo acadÃ©mico apropiado
- Patrones gramaticales

### 3. Coherence & Cohesion (15%)
- Fluidez argumentativa
- Conectividad entre secciones
- Consistencia narrativa
- Flujo lÃ³gico

### 4. Reproducibility (20%)
- Claridad metodolÃ³gica
- Disponibilidad de datos
- Disponibilidad de cÃ³digo
- EspecificaciÃ³n de parÃ¡metros

### 5. References & Citations (15%)
- Formato de citas
- Calidad de referencias
- Densidad de citas
- Recencia de referencias

### 6. Scientific Quality (20%)
- Novedad y originalidad
- Rigor metodolÃ³gico
- Significado de resultados
- ContribuciÃ³n teÃ³rica

## ConfiguraciÃ³n

El archivo `config/default.yaml` permite personalizar:

- Pesos de evaluaciÃ³n por pilar
- ConfiguraciÃ³n del LLM
- Niveles de logging
- ConfiguraciÃ³n de base de datos

## Testing

Ejecutar tests:
```bash
pytest tests/
```

## Estructura del Proyecto

```
src/
â”œâ”€â”€ models/           # Modelos de datos
â”œâ”€â”€ modules/          # MÃ³dulos de evaluaciÃ³n
â”œâ”€â”€ utils/            # Utilidades (parser PDF, etc.)
â”œâ”€â”€ config.py         # Sistema de configuraciÃ³n
â””â”€â”€ main.py          # Punto de entrada principal

data/
â”œâ”€â”€ sample_paper_1.txt    # Paper de muestra
â”œâ”€â”€ sample_paper_2.txt    # Paper de muestra
â””â”€â”€ ground_trouth_scores.csv  # Scores de referencia

config/
â””â”€â”€ default.yaml      # ConfiguraciÃ³n por defecto
```

## Formato de Salida

El sistema genera un reporte con:
- Score general (0.0 - 1.0)
- Scores por pilar
- Feedback detallado
- InformaciÃ³n del paper

Ejemplo:
```
============================================================
ðŸ“Š pAIper Check Evaluation Results
============================================================
ðŸ“„ Paper: Machine Learning Applications in Healthcare
ðŸŽ¯ Overall Score: 0.78/1.0

ðŸ“‹ Pillar Scores:
  â€¢ Structure & Completeness: 0.80 (weight: 0.15)
  â€¢ Linguistic Quality: 0.70 (weight: 0.15)
  â€¢ Coherence & Cohesion: 0.75 (weight: 0.15)
  â€¢ Reproducibility: 0.60 (weight: 0.20)
  â€¢ References & Citations: 0.85 (weight: 0.15)
  â€¢ Scientific Quality: 0.80 (weight: 0.20)

ðŸ’¬ Feedback Summary:
  â€¢ Structure & Completeness: Good structure with all essential sections present.
  â€¢ Linguistic Quality: Some informal language detected. Maintain formal academic tone.
  â€¢ Coherence & Cohesion: Good coherence with logical flow and consistent narrative.
  â€¢ Reproducibility: Data availability information is missing. Specify how data can be accessed.
  â€¢ References & Citations: Good reference quality with appropriate citation density.
  â€¢ Scientific Quality: Strong scientific quality with clear contributions and rigorous methodology.

âœ… Evaluation complete!
```

## PrÃ³ximos Pasos

- IntegraciÃ³n con MongoDB para almacenamiento persistente
- Interfaz web para evaluaciÃ³n interactiva
- API REST para integraciÃ³n con otros sistemas
- Mejoras en el parser de PDF para mayor precisiÃ³n
- IntegraciÃ³n con LLMs para evaluaciÃ³n mÃ¡s avanzada
