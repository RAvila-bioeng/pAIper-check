import pandas as pd
import pytest
import sys
from pathlib import Path

# Add src to path for imports
sys.path.append(str(Path(__file__).parent.parent / 'src'))

from utils.pdf_parser import parse_paper_from_pdf
from modules import (
    check_structure, check_linguistics, check_cohesion,
    check_reproducibility, check_references, check_quality
)

# Define the location of the ground truth data
GROUND_TRUTH_FILE = 'data/ground_trouth_scores.csv'
PAPER_FILE_PATH = 'data/sample_paper_1.txt'
# Tolerance for floating point comparison (e.g., allow a difference of 0.15)
SCORE_TOLERANCE = 0.15 

# Load all expected scores once for the test module
@pytest.fixture(scope='module')
def ground_truth_data():
    """Loads the ground truth CSV data into a pandas DataFrame."""
    try:
        return pd.read_csv(GROUND_TRUTH_FILE)
    except FileNotFoundError:
        pytest.fail(f"Ground truth file not found at: {GROUND_TRUTH_FILE}")

@pytest.fixture(scope='module')
def sample_paper():
    """Load the sample paper for testing."""
    paper_path = Path(PAPER_FILE_PATH)
    if not paper_path.exists():
        pytest.skip(f"Sample paper not found at: {PAPER_FILE_PATH}")
    
    # For text files, we'll read directly
    with open(paper_path, 'r', encoding='utf-8') as f:
        text = f.read()
    
    from models.paper import Paper
    return Paper(raw_text=text)

def test_pillar_score_matches_ground_truth(ground_truth_data, sample_paper):
    """
    Tests if the score generated by each evaluation module is close to the expected score.
    """
    
    # Map pillar names to evaluation functions
    evaluation_modules = {
        'check_structure': check_structure,
        'check_linguistics': check_linguistics,
        'check_cohesion': check_cohesion,
        'check_reproducibility': check_reproducibility,
        'check_references': check_references,
        'check_quality': check_quality
    }
    
    for index, row in ground_truth_data.iterrows():
        pillar_name = row['pillar_name']
        expected_score = row['expected_score']
        expected_keywords = [kw.strip() for kw in row['expected_feedback_keywords'].split(',')]

        # Skip if pillar module doesn't exist
        if pillar_name not in evaluation_modules:
            pytest.skip(f"Evaluation module {pillar_name} not found")

        # Run the actual evaluation using the module
        module = evaluation_modules[pillar_name]
        result = module.evaluate(sample_paper)
        
        actual_score = result['score']
        actual_feedback = result['feedback']

        # Check 3a: Score numerical validation (using tolerance)
        assert actual_score == pytest.approx(expected_score, abs=SCORE_TOLERANCE), \
            f"❌ FAILURE on {pillar_name}: Actual score {actual_score} is not close to expected {expected_score}."

        # Check 3b: Feedback keyword validation (at least one keyword should be present)
        keyword_found = any(keyword.lower() in actual_feedback.lower() for keyword in expected_keywords)
        assert keyword_found, \
            f"❌ FAILURE on {pillar_name}: Feedback '{actual_feedback}' is missing expected keywords {expected_keywords}."

def test_all_modules_return_valid_scores(sample_paper):
    """Test that all evaluation modules return valid scores between 0 and 1."""
    
    modules = [
        check_structure, check_linguistics, check_cohesion,
        check_reproducibility, check_references, check_quality
    ]
    
    for module in modules:
        result = module.evaluate(sample_paper)
        score = result['score']
        feedback = result['feedback']
        
        assert 0.0 <= score <= 1.0, f"Score {score} is not between 0 and 1 for {module.__name__}"
        assert isinstance(feedback, str), f"Feedback is not a string for {module.__name__}"
        assert len(feedback) > 0, f"Feedback is empty for {module.__name__}"